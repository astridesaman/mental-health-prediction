{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23954fa",
   "metadata": {},
   "source": [
    "# Mental Health Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e784224",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9cb4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MentalDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb08ec85",
   "metadata": {},
   "source": [
    "### Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9c585fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LinearBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    Modèle linéaire de référence (Logistic Regression).\n",
    "    Une seule couche + sigmoid.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "class MentalHealthModelNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Réseau de neurones simple pour classification binaire (dépression oui/non)\n",
    "    Sortie : probabilité entre 0 et 1\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid(),  # probabilité\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c47bb1",
   "metadata": {},
   "source": [
    "### Trainer Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e62a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "from src.dataset import MentalDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# 1. Chargement & prétraitement des données\n",
    "\n",
    "def load_and_preprocess(path: str):\n",
    "    \"\"\"\n",
    "    Charge train.csv, applique le prétraitement et renvoie :\n",
    "    X_train, y_train, X_val, y_val, mean, std, feature_cols\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Cible\n",
    "    target_col = \"Depression\"\n",
    "\n",
    "    # Colonnes inutiles\n",
    "    df = df.drop(columns=[\"id\", \"Name\"])\n",
    "\n",
    "    print(df)\n",
    "\n",
    "    # Colonnes numériques\n",
    "    num_cols = [\n",
    "        \"Age\",\n",
    "        \"Academic Pressure\",\n",
    "        \"Work Pressure\",\n",
    "        \"CGPA\",\n",
    "        \"Study Satisfaction\",\n",
    "        \"Job Satisfaction\",\n",
    "        \"Work/Study Hours\",\n",
    "        \"Financial Stress\",\n",
    "    ]\n",
    "\n",
    "    # Colonnes catégorielles\n",
    "    cat_cols = [\n",
    "        \"Gender\",\n",
    "        \"City\",\n",
    "        \"Working Professional or Student\",\n",
    "        \"Profession\",\n",
    "        \"Sleep Duration\",\n",
    "        \"Dietary Habits\",\n",
    "        \"Degree\",\n",
    "        \"Have you ever had suicidal thoughts ?\",\n",
    "        \"Family History of Mental Illness\",\n",
    "    ]\n",
    "\n",
    "    # Séparer X / y\n",
    "    y = df[target_col].astype(\"float32\").values\n",
    "    X = df.drop(columns=[target_col])\n",
    "\n",
    "    # Valeurs manquantes\n",
    "    X[num_cols] = X[num_cols].fillna(X[num_cols].median())\n",
    "    X[cat_cols] = X[cat_cols].fillna(\"Unknown\")\n",
    "\n",
    "    # One-hot encoding\n",
    "    X = pd.get_dummies(X, columns=cat_cols)\n",
    "\n",
    "    # Garder la liste des colonnes (pour test plus tard)\n",
    "    feature_cols = X.columns.tolist()\n",
    "\n",
    "    # Conversion en numpy\n",
    "    X = X.astype(\"float32\").values\n",
    "\n",
    "    # Split train / val (80 / 20)\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    split = int(0.8 * len(X))\n",
    "\n",
    "    train_idx = indices[:split]\n",
    "    val_idx = indices[split:]\n",
    "\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Standardisation\n",
    "    mean = X_train.mean(axis=0, keepdims=True)\n",
    "    std = X_train.std(axis=0, keepdims=True) + 1e-8\n",
    "\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_val = (X_val - mean) / std\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, mean, std, feature_cols\n",
    "\n",
    "# 4. Trainer \n",
    "\n",
    "@dataclass\n",
    "class MentalHealthTrainer:\n",
    "    batch_size: int\n",
    "    n_epochs: int\n",
    "    eval_samples: int  # nb d'échantillons pour l'éval train/val\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        train_set: MentalDataset,\n",
    "        val_set: MentalDataset,\n",
    "        optimizer: optim.Optimizer,\n",
    "        device: torch.device,\n",
    "        model_name: str = \"model\",\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Entraîne UN modèle et renvoie la meilleure accuracy validation.\n",
    "        Sauvegarde les meilleurs poids dans best_model_weights_{model_name}.pt\n",
    "        \"\"\"\n",
    "        model.to(device)\n",
    "\n",
    "        n_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"\\n=== Training {model_name} ({n_params:,} params) ===\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        best_val = 0.0\n",
    "        patience = 3\n",
    "        counter = 0\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # ---- TRAIN ----\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "            n_train = 0\n",
    "\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(X_batch)          # (B,1) dans [0,1]\n",
    "                loss = F.binary_cross_entropy(preds, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                bs = X_batch.size(0)\n",
    "                with torch.no_grad():\n",
    "                    predicted = (preds >= 0.5).float()\n",
    "                    acc = (predicted == y_batch).float().mean().item()\n",
    "\n",
    "                train_loss += loss.item() * bs\n",
    "                train_acc += acc * bs\n",
    "                n_train += bs\n",
    "\n",
    "            train_loss /= n_train\n",
    "            train_acc /= n_train\n",
    "\n",
    "            # ---- EVAL ----\n",
    "            metrics_train = self.eval(model, train_set, device)\n",
    "            metrics_val = self.eval(model, val_set, device)\n",
    "            val_acc = metrics_val[\"accuracy\"]\n",
    "\n",
    "            print(\n",
    "                f\"[{model_name}] Epoch {epoch+1:02d} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "                f\"Eval Train Acc: {metrics_train['accuracy']:.4f} | \"\n",
    "                f\"Eval Val Acc: {metrics_val['accuracy']:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Early stopping + sauvegarde des meilleurs poids\n",
    "            if val_acc > best_val:\n",
    "                best_val = val_acc\n",
    "                counter = 0\n",
    "                torch.save(model.state_dict(), f\"best_model_weights_{model_name}.pt\")\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping for {model_name}.\")\n",
    "                    break\n",
    "\n",
    "        return best_val\n",
    "\n",
    "    @torch.inference_mode\n",
    "    def eval(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        dataset: MentalDataset,\n",
    "        device: torch.device,\n",
    "    ) -> Dict[str, float]:\n",
    "        model.eval()\n",
    "\n",
    "        n = len(dataset)\n",
    "        n_samples = min(self.eval_samples, n)\n",
    "\n",
    "        sampler = RandomSampler(\n",
    "            dataset,\n",
    "            replacement=False,\n",
    "            num_samples=n_samples,\n",
    "        )\n",
    "\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=sampler,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        all_acc: List[torch.Tensor] = []\n",
    "        all_loss: List[torch.Tensor] = []\n",
    "\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            preds = model(X_batch)\n",
    "            loss = F.binary_cross_entropy(preds, y_batch, reduction=\"none\")\n",
    "\n",
    "            predicted = (preds >= 0.5).float()\n",
    "            acc = (predicted == y_batch).float()\n",
    "\n",
    "            all_acc.append(acc)\n",
    "            all_loss.append(loss)\n",
    "\n",
    "        acc_tensor = torch.cat(all_acc).mean()\n",
    "        loss_tensor = torch.cat(all_loss).mean()\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": float(acc_tensor),\n",
    "            \"loss\": float(loss_tensor),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73941737",
   "metadata": {},
   "source": [
    "### Prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "239b81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# mêmes listes que dans load_and_preprocess\n",
    "NUM_COLS = [\n",
    "    \"Age\",\n",
    "    \"Academic Pressure\",\n",
    "    \"Work Pressure\",\n",
    "    \"CGPA\",\n",
    "    \"Study Satisfaction\",\n",
    "    \"Job Satisfaction\",\n",
    "    \"Work/Study Hours\",\n",
    "    \"Financial Stress\",\n",
    "]\n",
    "\n",
    "CAT_COLS = [\n",
    "    \"Gender\",\n",
    "    \"City\",\n",
    "    \"Working Professional or Student\",\n",
    "    \"Profession\",\n",
    "    \"Sleep Duration\",\n",
    "    \"Dietary Habits\",\n",
    "    \"Degree\",\n",
    "    \"Have you ever had suicidal thoughts ?\",\n",
    "    \"Family History of Mental Illness\",\n",
    "]\n",
    "\n",
    "DROP_COLS = [\"id\", \"Name\"]\n",
    "\n",
    "\n",
    "def preprocess_test(df_test: pd.DataFrame, feature_cols, mean, std) -> np.ndarray:\n",
    "    \"\"\"Applique le même prétraitement que pour le train,\n",
    "    puis aligne les colonnes sur feature_cols.\n",
    "    \"\"\"\n",
    "\n",
    "    # on enlève les colonnes inutiles\n",
    "    df = df_test.drop(columns=DROP_COLS)\n",
    "\n",
    "    # valeurs manquantes\n",
    "    df[NUM_COLS] = df[NUM_COLS].fillna(df[NUM_COLS].median())\n",
    "    df[CAT_COLS] = df[CAT_COLS].fillna(\"Unknown\")\n",
    "\n",
    "    # one-hot\n",
    "    df = pd.get_dummies(df, columns=CAT_COLS)\n",
    "\n",
    "    # réaligner les colonnes sur celles du train\n",
    "    df = df.reindex(columns=feature_cols, fill_value=0.0)\n",
    "\n",
    "    X_test = df.astype(\"float32\").values\n",
    "\n",
    "    # standardisation avec mean/std du train\n",
    "    X_test = (X_test - mean) / (std + 1e-8)\n",
    "\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8b7cd",
   "metadata": {},
   "source": [
    "### Evaluation et comparaison des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3c71176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training linear (356 params) ===\n",
      "[linear] Epoch 01 | Train Loss: 0.4948 Acc: 0.8976 | Eval Train Acc: 0.9287 | Eval Val Acc: 0.9327\n",
      "[linear] Epoch 02 | Train Loss: 0.3684 Acc: 0.9336 | Eval Train Acc: 0.9313 | Eval Val Acc: 0.9344\n",
      "[linear] Epoch 03 | Train Loss: 0.3488 Acc: 0.9362 | Eval Train Acc: 0.9354 | Eval Val Acc: 0.9392\n",
      "[linear] Epoch 04 | Train Loss: 0.3419 Acc: 0.9370 | Eval Train Acc: 0.9385 | Eval Val Acc: 0.9378\n",
      "[linear] Epoch 05 | Train Loss: 0.3387 Acc: 0.9370 | Eval Train Acc: 0.9350 | Eval Val Acc: 0.9368\n",
      "[linear] Epoch 06 | Train Loss: 0.3372 Acc: 0.9371 | Eval Train Acc: 0.9394 | Eval Val Acc: 0.9403\n",
      "[linear] Epoch 07 | Train Loss: 0.3357 Acc: 0.9369 | Eval Train Acc: 0.9397 | Eval Val Acc: 0.9356\n",
      "[linear] Epoch 08 | Train Loss: 0.3146 Acc: 0.9368 | Eval Train Acc: 0.9345 | Eval Val Acc: 0.9411\n",
      "[linear] Epoch 09 | Train Loss: 0.2527 Acc: 0.9371 | Eval Train Acc: 0.9353 | Eval Val Acc: 0.9352\n",
      "[linear] Epoch 10 | Train Loss: 0.1952 Acc: 0.9373 | Eval Train Acc: 0.9405 | Eval Val Acc: 0.9378\n",
      "[linear] Epoch 11 | Train Loss: 0.1578 Acc: 0.9380 | Eval Train Acc: 0.9365 | Eval Val Acc: 0.9385\n",
      "Early stopping for linear.\n",
      "\n",
      "=== Training nn (53,889 params) ===\n",
      "[nn] Epoch 01 | Train Loss: 0.1783 Acc: 0.9298 | Eval Train Acc: 0.9426 | Eval Val Acc: 0.9361\n",
      "[nn] Epoch 02 | Train Loss: 0.1596 Acc: 0.9374 | Eval Train Acc: 0.9405 | Eval Val Acc: 0.9362\n",
      "[nn] Epoch 03 | Train Loss: 0.1570 Acc: 0.9380 | Eval Train Acc: 0.9408 | Eval Val Acc: 0.9377\n",
      "[nn] Epoch 04 | Train Loss: 0.1550 Acc: 0.9392 | Eval Train Acc: 0.9425 | Eval Val Acc: 0.9377\n",
      "[nn] Epoch 05 | Train Loss: 0.1532 Acc: 0.9396 | Eval Train Acc: 0.9446 | Eval Val Acc: 0.9394\n",
      "[nn] Epoch 06 | Train Loss: 0.1514 Acc: 0.9399 | Eval Train Acc: 0.9462 | Eval Val Acc: 0.9389\n",
      "[nn] Epoch 07 | Train Loss: 0.1514 Acc: 0.9402 | Eval Train Acc: 0.9429 | Eval Val Acc: 0.9387\n",
      "[nn] Epoch 08 | Train Loss: 0.1499 Acc: 0.9405 | Eval Train Acc: 0.9440 | Eval Val Acc: 0.9378\n",
      "Early stopping for nn.\n",
      "\n",
      "=== Model comparison (validation accuracy) ===\n",
      "+---------+--------------------+\n",
      "| Model   | Val Accuracy       |\n",
      "+---------+--------------------+\n",
      "| linear  |  94.11%            |\n",
      "| nn      |  93.94%            |\n",
      "+---------+--------------------+\n",
      "\n",
      "Best model: linear (94.11% val accuracy)\n",
      "Saved best model in mental_health_model.pt\n"
     ]
    }
   ],
   "source": [
    "from src.trainer import MentalHealthTrainer, load_and_preprocess\n",
    "from src.dataset import MentalDataset\n",
    "from src.model import LinearBaseline, MentalHealthModelNN\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1) Charger & prétraiter\n",
    "    X_train, y_train, X_val, y_val, mean, std, feature_cols = load_and_preprocess(\n",
    "        \"./data/train.csv\"\n",
    "    )\n",
    "\n",
    "    train_set = MentalDataset(X_train, y_train)\n",
    "    val_set = MentalDataset(X_val, y_val)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    trainer = MentalHealthTrainer(\n",
    "        batch_size=64,\n",
    "        n_epochs=25,\n",
    "        eval_samples=10_000,\n",
    "    )\n",
    "\n",
    "    # 2) Définir les modèles à comparer\n",
    "    models = {\n",
    "        \"linear\": LinearBaseline(input_dim),\n",
    "        \"nn\": MentalHealthModelNN(input_dim),\n",
    "    }\n",
    "\n",
    "    val_scores = {}\n",
    "\n",
    "    # 3) Entraîner chaque modèle et stocker la meilleure val accuracy\n",
    "    for name, model in models.items():\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        best_val_acc = trainer.train(\n",
    "            model=model,\n",
    "            train_set=train_set,\n",
    "            val_set=val_set,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            model_name=name,\n",
    "        )\n",
    "        val_scores[name] = best_val_acc\n",
    "\n",
    "    # 4) Afficher un tableau de comparaison\n",
    "    print(\"\\n=== Model comparison (validation accuracy) ===\")\n",
    "    print(\"+---------+--------------------+\")\n",
    "    print(\"| Model   | Val Accuracy       |\")\n",
    "    print(\"+---------+--------------------+\")\n",
    "    for name, acc in val_scores.items():\n",
    "        print(f\"| {name:<7} | {acc*100:>6.2f}%            |\")\n",
    "    print(\"+---------+--------------------+\")\n",
    "\n",
    "    # 5) Déterminer le meilleur modèle\n",
    "    best_model_name = max(val_scores, key=val_scores.get)\n",
    "    best_val = val_scores[best_model_name]\n",
    "    print(f\"\\nBest model: {best_model_name} ({best_val*100:.2f}% val accuracy)\")\n",
    "\n",
    "    # 6) Sauvegarder un checkpoint \"global\" pour predict.py\n",
    "    #    On recharge les poids du meilleur modèle et on les stocke dans un seul fichier.\n",
    "    best_state_dict = torch.load(f\"best_model_weights_{best_model_name}.pt\", map_location=\"cpu\")\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_type\": best_model_name,   # \"linear\" ou \"nn\"\n",
    "            \"state_dict\": best_state_dict,\n",
    "            \"mean\": mean,\n",
    "            \"std\": std,\n",
    "            \"feature_cols\": feature_cols,\n",
    "        },\n",
    "        \"mental_health_model.pt\",\n",
    "    )\n",
    "    print(\"Saved best model in mental_health_model.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
